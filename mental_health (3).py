# -*- coding: utf-8 -*-
"""Mental health

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zjecxdJ_ZMOJv1vdWHhtzXwRoYAK64ey
"""

# standard libraries
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import warnings
warnings.simplefilter('ignore')
import gc

# visualisation
import seaborn as sns
from matplotlib import pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

# dataprep

# tensorflow libraries
import tensorflow as tf
from tensorflow import keras
from tqdm.keras import TqdmCallback
import keras_tuner as kt

# load data
file ='/kaggle/input/mental-health-corpus/mental_health.csv'
df = pd.read_csv('/content/mental_health.csv.csv')
# drop NULLs
df.dropna(inplace=True)
# view
df.head()
# view
df.head()

import sys
import re

def clean_text(text):
    # Lowercase the text
    text = text.lower()
    # Remove URLs
    text = re.sub(r'http\S+|www\S+', '', text)
    # Remove special characters and digits
    text = re.sub(r'[^a-z\s]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

# clean / preprocess text data - quickly
# Apply the clean_text function to the 'text' column of the DataFrame
df['text'] = df['text'].apply(clean_text)

# clean / preprocess text data - quickly
# Apply the clean_text function to the 'text' column of the DataFrame
df['text'] = df['text'].apply(clean_text)

# The following line was causing a TypeError because clean_text expects only one argument (the text string)
# df = clean_text(df,'text')

# Remove the incorrect line to fix the error. The line above already performs the intended cleaning.

# label distribution
fig = plt.figure(figsize=(6, 6))
plt.title("label distribution")
sns.countplot(x=df['label'],palette='pastel')
fig.tight_layout()
plt.show()

# vectorize
# Changed min_df from 0 to 1, as 0 is not a valid integer for min_df
vectorizer = CountVectorizer(min_df=1, lowercase=True)
vectorizer.fit(df['text'])
# vectorizer.vocabulary_

# feature engineering
sentences = df['text'].values
y = df['label'].values

# train-test split [80-20]
sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.20, random_state=42)

# vectorize (again!)
vectorizer.fit(sentences_train)
x_train = vectorizer.transform(sentences_train)
x_test  = vectorizer.transform(sentences_test)

# params
input_dim = x_train.shape[1]  # Number of features
batch_size = np.ceil(len(sentences) / 256).astype(int)
output_shape = 1

# simple keras model
model = keras.models.Sequential()
model.add(keras.layers.Dense(batch_size, input_dim=input_dim, activation='relu'))
model.add(keras.layers.Dropout(0.2))
model.add(keras.layers.Dense(output_shape, activation='sigmoid'))

# model compile
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

# tensorflow libraries
import tensorflow as tf
from tensorflow import keras
from tqdm.keras import TqdmCallback

# Ensure keras_tuner is installed
# Now import keras_tuner
import keras_tuner as kt

# standard libraries
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file blabla(e.g. pd.read_csv)
import warnings
warnings.simplefilter('ignore')
import gc

# visualisation
import seaborn as sns
from matplotlib import pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split

# dataprep

# tensorflow libraries
import tensorflow as tf
from tensorflow import keras
from tqdm.keras import TqdmCallback
# Import keras_tuner here after ensuring it's installed
import keras_tuner as kt

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
import numpy as np

# Load example data
data = load_iris()
X = data.data
y = data.target

# Split into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Now this will work
input_dim = x_train.shape[1]

# tensorflow libraries
import tensorflow as tf
from tensorflow import keras
from tqdm.keras import TqdmCallback
import keras_tuner as kt # Move this import here

# instantiate the tuner and perform hypertuning

# Define the model_builder function first, as it's used by kt.Hyperband
# This function should build and compile a Keras model and return it.
# Here is a basic example based on your model definition:
def model_builder(hp, input_dim): # Add input_dim as an argument
    model = keras.models.Sequential()
    # Tune the number of units in the first Dense layer
    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)
    # Use the passed input_dim
    model.add(keras.layers.Dense(units=hp_units, input_dim=input_dim, activation='relu'))
    model.add(keras.layers.Dropout(0.2))
    model.add(keras.layers.Dense(1, activation='sigmoid'))

    # Tune the learning rate for the optimizer
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])
    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
    return model

# Define input_dim before initializing the tuner
input_dim = x_train.shape[1]

# Pass the input_dim to the model_builder when initializing the tuner
tuner = kt.Hyperband(
    hypermodel=lambda hp: model_builder(hp, input_dim=input_dim), # Pass input_dim here
    objective='val_accuracy',
    max_epochs=15,
    factor=3,
    directory='my_dir',
    project_name='intro_to_kt')

# early stopping
stop_early = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)

# tuner
tuner.search(x_train, y_train, epochs=15, validation_data=(x_test, y_test), callbacks=[stop_early])

# Get the optimal hyperparameters
best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
""")

# Build the model with the optimal hyperparameters and train it on the data for 50 epochs
model_base = tuner.hypermodel.build(best_hps)
base_hist = model_base.fit(x_train, y_train
                           ,epochs=15
                           ,validation_data=(x_test, y_test)
                           ,verbose=0
                           ,callbacks=[TqdmCallback(verbose=0)])
val_acc_per_epoch = base_hist.history['val_accuracy']
best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1
print('\nBest epoch: %d' % (best_epoch,))

# hyper tuned model
hypermodel = tuner.hypermodel.build(best_hps)
# model summary
keras.utils.plot_model(hypermodel,show_shapes=True,show_dtype=True)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import keras_tuner as kt

def model_builder(hp):
    model = keras.Sequential()
    model.add(layers.Flatten(input_shape=(28, 28)))  # Adjust for your dataset shape

    # Tune number of units in the first Dense layer
    hp_units = hp.Int('units', min_value=32, max_value=512, step=32)
    model.add(layers.Dense(units=hp_units, activation='relu'))
    model.add(layers.Dense(10, activation='softmax'))

    # Tune the learning rate
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])

    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    return model

tuner = kt.Hyperband(model_builder,
                     objective='val_accuracy',
                     max_epochs=10,
                     factor=3,
                     directory='my_dir',
                     project_name='intro_to_kt')

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
x_train = x_train / 255.0
x_test = x_test / 255.0

tuner.search(x_train, y_train, epochs=10, validation_split=0.2)

# Get the optimal hyperparameters
best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
""")

# Get the optimal hyperparameters
best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]

print(f"""
The hyperparameter search is complete. The optimal number of units in the first densely-connected
layer is {best_hps.get('units')} and the optimal learning rate for the optimizer
is {best_hps.get('learning_rate')}.
""")

# Retrain the model
# Define hypermodel here before using it
hypermodel = tuner.hypermodel.build(best_hps)

# Define the number of epochs for retraining
# Using a fixed number or you could potentially get the best epoch from tuner.search history if needed.
# For this fix, we'll use a reasonable fixed number of epochs.
retrain_epochs = 20 # Or any other suitable number

hist_tuned = hypermodel.fit(x_train, y_train
                            ,epochs=retrain_epochs # Use the defined variable
                            ,validation_data=(x_test, y_test)
                            ,verbose=0
                            ,callbacks=[TqdmCallback(verbose=0)])

# Assuming you've already gotten best_hps from tuner
hypermodel = tuner.hypermodel.build(best_hps)

hist_tuned = hypermodel.fit(x_train, y_train,
                            epochs=10,  # or best_epoch
                            validation_data=(x_test, y_test))

acc = hist_tuned.history['accuracy']
val_acc = hist_tuned.history['val_accuracy']
loss = hist_tuned.history['loss']
val_loss = hist_tuned.history['val_loss']

acc = '{:.2%}'.format(hist_tuned.history['accuracy'][-1])
print(f"Accuracy of our hyper-tuned model is {acc}")

# predictions - test data
import numpy as np # Ensure numpy is imported
import pandas as pd # Ensure pandas is imported

predictions = hypermodel.predict(x_test)
# round predictions
# Note: Since the model is trained on MNIST (multi-class classification),
# rounding predictions from a sigmoid output is incorrect.
# For MNIST, you should take the argmax of the predictions.
# If your original problem is binary classification, keep the rounding.
# Assuming the traceback was from the MNIST section based on x_test shape (28, 28):
y_pred = np.argmax(predictions, axis=1) # Use argmax for multi-class

# Need to import pandas if not already imported in this session
# import pandas as pd # Moved to the top
# import numpy as np # Ensure numpy is imported for argmax if not already # Moved to the top

# test data - dataframe with predictions
# Note: sentences_test and y_test were from the mental health dataset,
# but the x_test used for prediction is from the MNIST dataset.
# This will likely cause dimension mismatch or incorrect results.
# If you intended to predict on the original text data test set:
# 1. Ensure x_test in this cell is the vectorized text data test set.
# 2. Ensure y_test in this cell corresponds to the text data test set labels.
# 3. Ensure the hypermodel was trained on the vectorized text data, not MNIST.

# Assuming you want to create a DataFrame for the MNIST test set:
# Need to handle the original labels y_test which are also from MNIST in this context.
df_test = pd.DataFrame({'label':y_test
                        ,'label_pred':y_pred})

# If you meant to use the original mental health data test set (sentences_test, y_test):
# You would need to make sure the model is trained and predicting on that data.
# Assuming the traceback originated after running the MNIST cells,
# sentences_test and y_test might be undefined or hold previous values.
# If you want to display results for the MNIST test set:
# df_test = pd.DataFrame({'actual_label': y_test, 'predicted_label': y_pred})


# view
df_test.head()

# save the model
hypermodel.save('MentalHealthLanguageModel.h5')

# Save the Keras model with the correct filename
# The previous cell already saved it as 'MentalHealthLanguageModel.h5'
# hypermodel.save('MentalHealthLanguageModel.h5')

# Save the vectorizer object using pickle
import pickle # Import the pickle module

try:
    with open('vectorizer.pkl', 'wb') as f:
        pickle.dump(vectorizer, f)
    print("Vectorizer saved as vectorizer.pkl")
except Exception as e:
    print(f"Error saving vectorizer: {e}")


from google.colab import files

# Download the saved model file and the saved vectorizer file
try:
    files.download("MentalHealthLanguageModel.h5")
    print("Model file MentalHealthLanguageModel.h5 download initiated.")
except FileNotFoundError:
    print("Error: Model file MentalHealthLanguageModel.h5 not found. Did it save correctly?")
except Exception as e:
    print(f"Error downloading model file: {e}")

try:
    files.download("vectorizer.pkl")
    print("Vectorizer file vectorizer.pkl download initiated.")
except FileNotFoundError:
    print("Error: Vectorizer file vectorizer.pkl not found. Did it save correctly?")
except Exception as e:
    print(f"Error downloading vectorizer file: {e}")

# app.py

import streamlit as st
import joblib

# Load model and vectorizer
# Note: You saved the model as 'MentalHealthLanguageModel.h5' and the vectorizer as 'vectorizer.pkl'
# Your code is trying to load 'mental_health_model.pkl'. This needs to be corrected.
try:
    # Assuming you want to load the Keras model saved as .h5
    # Streamlit will typically serve this app, and you'd need to load the Keras model
    # directly using tensorflow/keras. joblib is usually for scikit-learn models or other objects.
    # If you intended to save a scikit-learn model, you would need to train one and save it.
    # For this fix, let's assume you want to load the Keras model and the vectorizer.
    from tensorflow import keras
    model = keras.models.load_model("MentalHealthLanguageModel.h5")
    print("Keras model loaded successfully.")

    with open("vectorizer.pkl", 'rb') as f:
        vectorizer = joblib.load(f) # joblib can also load objects saved with pickle
    print("Vectorizer loaded successfully.")

except FileNotFoundError as e:
    st.error(f"Error loading model or vectorizer: {e}. Make sure the files are in the same directory as app.py")
    st.stop() # Stop the app if files are not found
except Exception as e:
    st.error(f"An unexpected error occurred during loading: {e}")
    st.stop() # Stop the app on other errors


# Streamlit app layout
st.title("ðŸ§  Mental Health Detection Chatbot")
st.write("Describe how you're feeling and I'll help detect any mental health concerns.")

# Input box
user_input = st.text_area("You:", "")

if st.button("Analyze"):
    if user_input.strip():
        # You will need to preprocess the user input the same way the training data was processed
        # Make sure clean_text function is available in this script or imported
        # Assuming clean_text function is defined or imported:
        try:
            # Placeholder for cleaning - ensure clean_text is available
            # cleaned_input = clean_text(user_input)
            # Assuming vectorizer expects a list of strings
            X = vectorizer.transform([user_input])

            # Predict using the Keras model
            prediction_proba = model.predict(X)

            # Since your original model was a binary classifier with sigmoid output,
            # you need to convert the probability to a class label (e.g., 0 or 1)
            # You might want to map these labels to meaningful text like "No Concern" or "Concern"
            predicted_class = (prediction_proba > 0.5).astype("int32")[0][0] # Assuming binary classification output

            # Map the predicted class to a meaningful label
            # You need to know what 0 and 1 correspond to in your original dataset labels.
            # For example:
            label_mapping = {0: "No apparent concern", 1: "Potential concern"}
            predicted_label_text = label_mapping.get(predicted_class, "Unknown label")

            st.write("ðŸ¤– Bot:", f"Based on your message, there is: **{predicted_label_text}**")

        except Exception as e:
            st.error(f"An error occurred during analysis: {e}")

    else:
        st.warning("Please enter a message.")
